{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chenjinghuang\\anaconda3\\envs\\TS-TCC\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records train: 11\n",
      "Number of records validation: 5\n",
      "Number of records test: 5\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import json\n",
    "\n",
    "h5_directory = '../data/h5'  # adapt if you used a different DOWNLOAD_PATH when running `make download_example`\n",
    "import torch\n",
    "import tempfile\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from dosed.utils import Compose\n",
    "from dosed.datasets import BalancedEventDataset as dataset\n",
    "from dosed.models import DOSED3 as model\n",
    "from dosed.datasets import get_train_validation_test\n",
    "from dosed.trainers import trainers\n",
    "from dosed.preprocessing import GaussianNoise, RescaleNormal, Invert\n",
    "\n",
    "seed = 2019\n",
    "train, validation, test = get_train_validation_test(h5_directory,\n",
    "                                                    percent_test=25,\n",
    "                                                    percent_validation=33,\n",
    "                                                    seed=seed)\n",
    "\n",
    "print(\"Number of records train:\", len(train))\n",
    "print(\"Number of records validation:\", len(validation))\n",
    "print(\"Number of records test:\", len(test))\n",
    "window = 10  # window duration in seconds\n",
    "ratio_positive = 0.5  # When creating the batch, sample containing at least one spindle will be drawn with that probability\n",
    "# 一个训练批次中，每个样本包含至少一个脑电活动（spindle）的概率\n",
    "fs = 32\n",
    "\n",
    "signals = [\n",
    "    {\n",
    "        'h5_path': '/eeg_0',\n",
    "        'fs': 64,\n",
    "        'processing': {\n",
    "            \"type\": \"clip_and_normalize\",\n",
    "            \"args\": {\n",
    "                    \"min_value\": -150,\n",
    "                \"max_value\": 150,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'h5_path': '/eeg_1',\n",
    "        'fs': 64,\n",
    "        'processing': {\n",
    "            \"type\": \"clip_and_normalize\",\n",
    "            \"args\": {\n",
    "                    \"min_value\": -150,\n",
    "                \"max_value\": 150,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "events = [\n",
    "    {\n",
    "        \"name\": \"spindle\",\n",
    "        \"h5_path\": \"spindle\",\n",
    "    },\n",
    "]\n",
    "dataset_parameters = {\n",
    "    \"h5_directory\": h5_directory,\n",
    "    \"signals\": signals,\n",
    "    \"events\": events,\n",
    "    \"window\": window,\n",
    "    \"fs\": fs,\n",
    "    \"ratio_positive\": ratio_positive,\n",
    "    \"n_jobs\": -1,  # Make use of parallel computing to extract and normalize signals from h5\n",
    "    \"cache_data\": True,  # by default will store normalized signals extracted from h5 in h5_directory + \"/.cache\" directory\n",
    "}\n",
    "\n",
    "dataset_validation = dataset(records=validation, **dataset_parameters)\n",
    "dataset_test = dataset(records=test, **dataset_parameters)\n",
    "\n",
    "# for training add data augmentation\n",
    "dataset_parameters_train = {\n",
    "    \"transformations\": Compose([\n",
    "        GaussianNoise(),\n",
    "        RescaleNormal(),\n",
    "        Invert(),\n",
    "    ])\n",
    "}\n",
    "dataset_parameters_train.update(dataset_parameters)\n",
    "dataset_train = dataset(records=train, **dataset_parameters_train)# inputsize=2,320,这里进行数据尺寸切割\n",
    "default_event_sizes = [0.7, 1, 1.3]#如何选取默认事件的大小，要检测的事件是脑电活动（spindle），大小约为1s\n",
    "k_max = 5\n",
    "kernel_size = 5\n",
    "probability_dropout = 0.1\n",
    "device = torch.device(\"cuda\")\n",
    "sampling_frequency = dataset_train.fs\n",
    "\n",
    "net_parameters = {\n",
    "    \"detection_parameters\": {\n",
    "        \"overlap_non_maximum_suppression\": 0.5,\n",
    "        \"classification_threshold\": 0.7\n",
    "    },\n",
    "    \"default_event_sizes\": [\n",
    "        default_event_size * sampling_frequency\n",
    "        for default_event_size in default_event_sizes\n",
    "    ],\n",
    "    \"k_max\": k_max,\n",
    "    \"kernel_size\": kernel_size,\n",
    "    \"pdrop\": probability_dropout,\n",
    "    \"fs\": sampling_frequency,   # just used to print architecture info with right time\n",
    "    \"input_shape\": dataset_train.input_shape,# 2，10s，采样率32hz\n",
    "    \"number_of_classes\": dataset_train.number_of_classes,# 1\n",
    "}\n",
    "net = model(**net_parameters)\n",
    "net = net.to(device)\n",
    "\n",
    "\n",
    "optimizer_parameters = {\n",
    "    \"lr\": 5e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "}\n",
    "loss_specs = {\n",
    "    \"type\": \"focal\",\n",
    "    \"parameters\": {\n",
    "        \"number_of_classes\": dataset_train.number_of_classes,\n",
    "        \"device\": device,\n",
    "    }\n",
    "}\n",
    "epochs = 20\n",
    "dataloader_parameters = {\n",
    "            \"num_workers\": 8,\n",
    "            \"shuffle\": True,\n",
    "            \"collate_fn\": 0.5,\n",
    "            \"pin_memory\": True,\n",
    "            \"batch_size\": 1\n",
    "        }\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, **dataloader_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 691200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.signals['83366db3-9096-440a-a460-9d9b8a1c6777.h5']['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers[\"adam\"](\n",
    "    net,\n",
    "    optimizer_parameters=optimizer_parameters,\n",
    "    loss_specs=loss_specs,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "best_net_train, best_metrics_train, best_threshold_train = trainer.train(\n",
    "    dataset_train,\n",
    "    dataset_validation,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 假设 best_threshold_train 是你要保存的超参数\n",
    "hyperparameters = {'best_threshold_train': best_threshold_train,\n",
    "                    'best_metrics_train': best_metrics_train}\n",
    "\n",
    "# 保存超参数\n",
    "with open('hyperparameters.pkl', 'wb') as f:\n",
    "    pickle.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dosed.functions import compute_metrics_dataset\n",
    "metrics_test = compute_metrics_dataset(\n",
    "    best_net_train,\n",
    "    dataset_test,\n",
    "    threshold=best_threshold_train,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dosed.functions import compute_metrics_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net_self = torch.load(r'D:\\Desktop\\research\\github repo\\dosed\\best_net_train(self)20.pth')\n",
    "metrics_self = compute_metrics_dataset(\n",
    "    best_net_self,\n",
    "    dataset_test,\n",
    "    threshold=best_threshold_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset(records=train, **dataset_parameters_train)# inputsize=2,320,这里进行数据尺寸切割\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train[0:2]\n",
    "train_test\n",
    "dataset_train_test = dataset(records=train_test, **dataset_parameters_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers[\"adam\"](\n",
    "    best_net_self,\n",
    "    optimizer_parameters=optimizer_parameters,\n",
    "    loss_specs=loss_specs,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_net_train, my_metrics_train, my_threshold_train = trainer.train(\n",
    "    dataset_train_test,\n",
    "    dataset_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_threshold_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_metrics_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_s = compute_metrics_dataset(\n",
    "    my_net_train,\n",
    "    dataset_test,\n",
    "    threshold=my_threshold_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "record = dataset_test.records[1]\n",
    "\n",
    "index_spindle = 30\n",
    "window_duration = 5\n",
    "\n",
    "# retrive spindle at the right index\n",
    "spindle_start = float(predictions[record][0][index_spindle][0]) / sampling_frequency\n",
    "spindle_end = float(predictions[record][0][index_spindle][1]) / sampling_frequency\n",
    "\n",
    "# center data window on annotated spindle \n",
    "start_window = spindle_start + (spindle_end - spindle_start) / 2 - window_duration\n",
    "stop_window = spindle_start + (spindle_end - spindle_start) / 2 + window_duration\n",
    "\n",
    "# Retrieve EEG data at right index\n",
    "index_start = int(start_window * sampling_frequency)#回程采样频率\n",
    "index_stop = int(stop_window * sampling_frequency)\n",
    "y = dataset_test.signals[record][\"data\"][0][index_start:index_stop]\n",
    "\n",
    "# Build corresponding time support\n",
    "t = start_window + np.cumsum(np.ones(index_stop - index_start) * 1 / sampling_frequency)\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.plot(t, y)\n",
    "plt.axvline(spindle_end)\n",
    "plt.axvline(spindle_start)\n",
    "plt.ylim([-1, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
